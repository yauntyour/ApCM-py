Using device: cuda
============================================================
🎯 训练策略：使用生成数据训练，真实数据测试
============================================================

🔧 生成训练数据...
生成了 2000 张训练图像
训练数据形状: torch.Size([2000, 1024])

📂 加载真实测试数据...
加载了 250 张真实测试图像
测试数据形状: torch.Size([250, 1024])

📊 参数设置:
  输入维度: 1024
  压缩维度: 256
  压缩率: 0.250 (4:1)
  批大小: 32
  训练轮数: 2000
  学习率: 0.001
  学习率调整: False
  网络层数: 6
  隐藏层维度: 512
  预测层维度: 512

🚀 开始训练（使用生成数据）...
Epoch 50/2000, Loss: 0.007631, Recon Loss: 0.007319, LR: 0.001000
Epoch 100/2000, Loss: 0.004040, Recon Loss: 0.003810, LR: 0.001000
Epoch 150/2000, Loss: 0.002831, Recon Loss: 0.002621, LR: 0.001000
Epoch 200/2000, Loss: 0.002857, Recon Loss: 0.002679, LR: 0.001000
Epoch 250/2000, Loss: 0.002545, Recon Loss: 0.002393, LR: 0.001000
Epoch 300/2000, Loss: 0.002483, Recon Loss: 0.002308, LR: 0.001000
Epoch 350/2000, Loss: 0.002228, Recon Loss: 0.002099, LR: 0.001000
Epoch 400/2000, Loss: 0.001991, Recon Loss: 0.001873, LR: 0.001000
Epoch 450/2000, Loss: 0.001869, Recon Loss: 0.001762, LR: 0.001000
Epoch 500/2000, Loss: 0.001902, Recon Loss: 0.001791, LR: 0.001000
Epoch 550/2000, Loss: 0.001904, Recon Loss: 0.001802, LR: 0.001000
Epoch 600/2000, Loss: 0.001795, Recon Loss: 0.001692, LR: 0.001000
Epoch 650/2000, Loss: 0.001737, Recon Loss: 0.001660, LR: 0.001000
Epoch 700/2000, Loss: 0.001742, Recon Loss: 0.001641, LR: 0.001000
Epoch 750/2000, Loss: 0.001645, Recon Loss: 0.001553, LR: 0.001000
Epoch 800/2000, Loss: 0.001665, Recon Loss: 0.001588, LR: 0.001000
Epoch 850/2000, Loss: 0.001585, Recon Loss: 0.001506, LR: 0.001000
Epoch 900/2000, Loss: 0.001581, Recon Loss: 0.001502, LR: 0.001000
Epoch 950/2000, Loss: 0.001521, Recon Loss: 0.001442, LR: 0.001000
Epoch 1000/2000, Loss: 0.001461, Recon Loss: 0.001394, LR: 0.001000
Epoch 1050/2000, Loss: 0.001495, Recon Loss: 0.001416, LR: 0.001000
Epoch 1100/2000, Loss: 0.001362, Recon Loss: 0.001298, LR: 0.001000
Epoch 1150/2000, Loss: 0.001279, Recon Loss: 0.001212, LR: 0.001000
Epoch 1200/2000, Loss: 0.001346, Recon Loss: 0.001280, LR: 0.001000
Epoch 1250/2000, Loss: 0.001295, Recon Loss: 0.001236, LR: 0.001000
Epoch 1300/2000, Loss: 0.001350, Recon Loss: 0.001287, LR: 0.001000
Epoch 1350/2000, Loss: 0.001253, Recon Loss: 0.001187, LR: 0.001000
Epoch 1400/2000, Loss: 0.001152, Recon Loss: 0.001089, LR: 0.001000
Epoch 1450/2000, Loss: 0.001143, Recon Loss: 0.001081, LR: 0.001000
Epoch 1500/2000, Loss: 0.001182, Recon Loss: 0.001126, LR: 0.001000
Epoch 1550/2000, Loss: 0.001112, Recon Loss: 0.001054, LR: 0.001000
Epoch 1600/2000, Loss: 0.001051, Recon Loss: 0.000998, LR: 0.001000
Epoch 1650/2000, Loss: 0.000967, Recon Loss: 0.000909, LR: 0.001000
Epoch 1700/2000, Loss: 0.001002, Recon Loss: 0.000949, LR: 0.001000
Epoch 1750/2000, Loss: 0.001023, Recon Loss: 0.000973, LR: 0.001000
Epoch 1800/2000, Loss: 0.001034, Recon Loss: 0.000981, LR: 0.001000
Epoch 1850/2000, Loss: 0.000871, Recon Loss: 0.000819, LR: 0.001000
Epoch 1900/2000, Loss: 0.000907, Recon Loss: 0.000857, LR: 0.001000
Epoch 1950/2000, Loss: 0.000905, Recon Loss: 0.000859, LR: 0.001000
Epoch 2000/2000, Loss: 0.000855, Recon Loss: 0.000806, LR: 0.001000

训练完成，学习率共调整了 0 次
最佳损失: 0.000757
加载最佳模型进行测试...

📈 绘制训练损失曲线...
Loss curves saved to assets/loss_curves_4Lr[False]n6h512ph512E2000.png

============================================================
🧪 开始测试（使用真实数据）...
============================================================
测试数据维度: torch.Size([250, 1024])
压缩后维度: torch.Size([250, 256])
压缩率: 0.250

📊 整体测试结果:
  重建 MAE: nan
  重建 MSE: nan
  PSNR: nan dB

📸 生成可视化对比图（真实数据 vs 重建结果）...
Loss curves saved to assets/Example_4Lr[False]n6h512ph512E2000.png

📈 每张测试图像的详细指标:
  图像 1: PSNR = 18.01 dB, MAE = 0.089568, MSE = 0.015819
  图像 2: PSNR = 16.91 dB, MAE = 0.103603, MSE = 0.020382
  图像 3: PSNR = 15.92 dB, MAE = 0.119398, MSE = 0.025603
  图像 4: PSNR = 20.41 dB, MAE = 0.065198, MSE = 0.009097
  图像 5: PSNR = 16.20 dB, MAE = 0.112305, MSE = 0.023976

============================================================
✅ 测试完成！
  训练数据: 2000 张生成图像
  测试数据: 250 张真实图像
  压缩效果: 1024 维 → 256 维
  最佳模型已保存到: models/best_model.pth
============================================================
