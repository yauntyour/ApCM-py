Using device: cuda
============================================================
🎯 训练策略：使用生成数据训练，真实数据测试
============================================================

🔧 生成训练数据...
生成了 2000 张训练图像
训练数据形状: torch.Size([2000, 1024])

📂 加载真实测试数据...
加载了 250 张真实测试图像
测试数据形状: torch.Size([250, 1024])

📊 参数设置:
  输入维度: 1024
  压缩维度: 128
  压缩率: 0.125 (8:1)
  批大小: 32
  训练轮数: 2000
  学习率: 0.001
  学习率调整: False
  网络层数: 12
  隐藏层维度: 512
  预测层维度: 512

🚀 开始训练（使用生成数据）...
Epoch 50/2000, Loss: 0.009288, Recon Loss: 0.008976, LR: 0.001000
Epoch 100/2000, Loss: 0.005227, Recon Loss: 0.005007, LR: 0.001000
Epoch 150/2000, Loss: 0.004375, Recon Loss: 0.004128, LR: 0.001000
Epoch 200/2000, Loss: 0.003742, Recon Loss: 0.003532, LR: 0.001000
Epoch 250/2000, Loss: 0.003242, Recon Loss: 0.003058, LR: 0.001000
Epoch 300/2000, Loss: 0.003091, Recon Loss: 0.002954, LR: 0.001000
Epoch 350/2000, Loss: 0.002914, Recon Loss: 0.002769, LR: 0.001000
Epoch 400/2000, Loss: 0.002647, Recon Loss: 0.002521, LR: 0.001000
Epoch 450/2000, Loss: 0.002752, Recon Loss: 0.002619, LR: 0.001000
Epoch 500/2000, Loss: 0.002301, Recon Loss: 0.002204, LR: 0.001000
Epoch 550/2000, Loss: 0.002511, Recon Loss: 0.002382, LR: 0.001000
Epoch 600/2000, Loss: 0.002326, Recon Loss: 0.002211, LR: 0.001000
Epoch 650/2000, Loss: 0.002184, Recon Loss: 0.002070, LR: 0.001000
Epoch 700/2000, Loss: 0.002104, Recon Loss: 0.001989, LR: 0.001000
Epoch 750/2000, Loss: 0.002022, Recon Loss: 0.001920, LR: 0.001000
Epoch 800/2000, Loss: 0.001978, Recon Loss: 0.001891, LR: 0.001000
Epoch 850/2000, Loss: 0.002108, Recon Loss: 0.002009, LR: 0.001000
Epoch 900/2000, Loss: 0.001985, Recon Loss: 0.001887, LR: 0.001000
Epoch 950/2000, Loss: 0.001871, Recon Loss: 0.001795, LR: 0.001000
Epoch 1000/2000, Loss: 0.001858, Recon Loss: 0.001779, LR: 0.001000
Epoch 1050/2000, Loss: 0.001758, Recon Loss: 0.001677, LR: 0.001000
Epoch 1100/2000, Loss: 0.001644, Recon Loss: 0.001556, LR: 0.001000
Epoch 1150/2000, Loss: 0.001515, Recon Loss: 0.001428, LR: 0.001000
Epoch 1200/2000, Loss: 0.001582, Recon Loss: 0.001504, LR: 0.001000
Epoch 1250/2000, Loss: 0.001428, Recon Loss: 0.001354, LR: 0.001000
Epoch 1300/2000, Loss: 0.001460, Recon Loss: 0.001393, LR: 0.001000
Epoch 1350/2000, Loss: 0.001276, Recon Loss: 0.001206, LR: 0.001000
Epoch 1400/2000, Loss: 0.001352, Recon Loss: 0.001285, LR: 0.001000
Epoch 1450/2000, Loss: 0.001320, Recon Loss: 0.001259, LR: 0.001000
Epoch 1500/2000, Loss: 0.001281, Recon Loss: 0.001210, LR: 0.001000
Epoch 1550/2000, Loss: 0.001180, Recon Loss: 0.001120, LR: 0.001000
Epoch 1600/2000, Loss: 0.001157, Recon Loss: 0.001094, LR: 0.001000
Epoch 1650/2000, Loss: 0.001153, Recon Loss: 0.001092, LR: 0.001000
Epoch 1700/2000, Loss: 0.000999, Recon Loss: 0.000935, LR: 0.001000
Epoch 1750/2000, Loss: 0.001033, Recon Loss: 0.000973, LR: 0.001000
Epoch 1800/2000, Loss: 0.000971, Recon Loss: 0.000915, LR: 0.001000
Epoch 1850/2000, Loss: 0.000923, Recon Loss: 0.000867, LR: 0.001000
Epoch 1900/2000, Loss: 0.000853, Recon Loss: 0.000794, LR: 0.001000
Epoch 1950/2000, Loss: 0.000897, Recon Loss: 0.000847, LR: 0.001000
Epoch 2000/2000, Loss: 0.000787, Recon Loss: 0.000735, LR: 0.001000

训练完成，学习率共调整了 0 次
最佳损失: 0.000747
加载最佳模型进行测试...

📈 绘制训练损失曲线...
Loss curves saved to assets/loss_curves_8Lr[False]n12h512ph512E2000.png

============================================================
🧪 开始测试（使用真实数据）...
============================================================
测试数据维度: torch.Size([250, 1024])
压缩后维度: torch.Size([250, 128])
压缩率: 0.125

📊 整体测试结果:
  重建 MAE: nan
  重建 MSE: nan
  PSNR: nan dB

📸 生成可视化对比图（真实数据 vs 重建结果）...
Loss curves saved to assets/Example_8Lr[False]n12h512ph512E2000.png

📈 每张测试图像的详细指标:
  图像 1: PSNR = 17.41 dB, MAE = 0.099542, MSE = 0.018153
  图像 2: PSNR = 16.68 dB, MAE = 0.107916, MSE = 0.021491
  图像 3: PSNR = 15.11 dB, MAE = 0.129565, MSE = 0.030806
  图像 4: PSNR = 19.25 dB, MAE = 0.080272, MSE = 0.011878
  图像 5: PSNR = 15.69 dB, MAE = 0.128121, MSE = 0.026987

============================================================
✅ 测试完成！
  训练数据: 2000 张生成图像
  测试数据: 250 张真实图像
  压缩效果: 1024 维 → 128 维
  最佳模型已保存到: models/best_model.pth
============================================================
